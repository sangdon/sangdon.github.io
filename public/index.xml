<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sangdon Park</title><link>http://localhost:1313/</link><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml"/><description>Sangdon Park</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 24 Oct 2023 00:00:00 +0000</lastBuildDate><image><url>http://localhost:1313/media/icon_hu_5e0546d98affa013.png</url><title>Sangdon Park</title><link>http://localhost:1313/</link></image><item><title>Uncertainty Quantification</title><link>http://localhost:1313/research/uq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/research/uq/</guid><description>&lt;p>Uncertainty quantification is a critical tool for building trustworthy AI. We consider the following key question:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Can we rigorously learn and quantify the uncertainty of AI models, e.g., Large Language Models (LLMs), price predictors, or drones, under distribution shift and adversarial manipulation?&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h3 id="on-goingpotential-projects">On-going/Potential Projects&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Selective Prediction&lt;/strong>: Improve selective prediction methods.&lt;/li>
&lt;li>&lt;strong>Conformal Prediction&lt;/strong>: Improve conformal prediction methods.&lt;/li>
&lt;li>&lt;strong>Calibration&lt;/strong>: Improve calibration methods.&lt;/li>
&lt;/ul>
&lt;h3 id="keywords">Keywords&lt;/h3>
&lt;ul>
&lt;li>Uncertainty Quantification&lt;/li>
&lt;li>Calibration&lt;/li>
&lt;li>Conformal Prediction&lt;/li>
&lt;li>Learning Theory&lt;/li>
&lt;li>Distribution Shift&lt;/li>
&lt;/ul>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://openreview.net/forum?id=BJxVI04YvB" target="_blank" rel="noopener">ICLR'20&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://proceedings.mlr.press/v108/park20b/park20b.pdf" target="_blank" rel="noopener">AISTATS'20&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openreview.net/forum?id=Qk-Wq5AIjpq" target="_blank" rel="noopener">ICLR'21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openreview.net/pdf?id=DhP9L8vIyLc" target="_blank" rel="noopener">ICLR'22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2204.07482" target="_blank" rel="noopener">arXiv'22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openreview.net/forum?id=s6ygs1UCOw1" target="_blank" rel="noopener">NeurIPS'22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.usenix.org/conference/usenixsecurity23/presentation/park" target="_blank" rel="noopener">Security'23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2307.09254" target="_blank" rel="noopener">NeurIPS'24&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2506.14067" target="_blank" rel="noopener">arXiv'25&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Machine Unlearning</title><link>http://localhost:1313/research/unlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/research/unlearning/</guid><description>&lt;p>Trust issues on Agentic AI would never end. Whenever we discover trust issues, how can we mitigate this? We leverage machine unlearning for this. To this end, we consider the following question.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>How to efficiently unlearn trust issues and certify this?&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h3 id="on-goingpotential-projects">On-going/Potential Projects&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Certified Unlearning&lt;/strong>: Certify unlearning in LLMs.&lt;/li>
&lt;li>&lt;strong>Efficient Unlearning&lt;/strong>: Unlearn efficiently with few gradient steps.&lt;/li>
&lt;/ul>
&lt;p>What&amp;rsquo;s your own project idea?&lt;/p>
&lt;h3 id="keywords">Keywords&lt;/h3>
&lt;ul>
&lt;li>Machine Unlearning&lt;/li>
&lt;li>LLMs&lt;/li>
&lt;li>Conformal Prediction&lt;/li>
&lt;li>Online Learning&lt;/li>
&lt;/ul></description></item><item><title>Red Teaming on Agentic AI</title><link>http://localhost:1313/research/redteaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/research/redteaming/</guid><description>&lt;p>It is the era of Agentic AI but Agentic AI still suffers from trustworthiness issues, including weakness on jailbreaking.
We advance the status of Agentic AI by debugging its issues via red teaming. In particular, we consider the following question:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>How to learn red agentic AI to debug target agentic AI continuously?&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>We exploit our knowledge and experience on Trustworthy Agentic AI
to build red Agentic AI for blue Agentic AI.&lt;/p>
&lt;h3 id="on-goingpotential-projects">On-going/Potential Projects&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Red Agent Learning&lt;/strong>: Learn red Agents for blue LLMs via RL.&lt;/li>
&lt;li>&lt;strong>Red Agent Continual Learning&lt;/strong>: Learn continual red teaming Agents.&lt;/li>
&lt;/ul>
&lt;p>Do you have creative ideas in building red Agentic AI?&lt;/p>
&lt;h3 id="keywords">Keywords&lt;/h3>
&lt;ul>
&lt;li>Red Teaming&lt;/li>
&lt;li>Agentic AI&lt;/li>
&lt;li>Reinforcement Learning&lt;/li>
&lt;/ul>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.darpa.mil/news/2025/aixcc-results" target="_blank" rel="noopener">AIxCC-Winner&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Trustworthy Robotics</title><link>http://localhost:1313/research/trustrobots/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/research/trustrobots/</guid><description>&lt;p>The brain of AI has been replaced by large language models (LLMs). The next challenge (or long-standing challenge) is how to build Physical AI? We attach this challenge by making Physical AI trustworthy. In particular, we consider the following question.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>How to build safe robots or humanoids that could live with us and even our kids?&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>Based on our previous experiments on trustworthy AI, we build ground breaking learning methods to achieve this goal.&lt;/p>
&lt;h3 id="on-goingpotential-projects">On-going/Potential Projects&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Safe Humanoid Learning&lt;/strong>: Learn safe humanoids.&lt;/li>
&lt;li>&lt;strong>Policy Calibration&lt;/strong>: Calibrate RL policies.&lt;/li>
&lt;li>&lt;strong>Safe RL with Certification&lt;/strong>: Guarantee safety in RL.&lt;/li>
&lt;li>&lt;strong>Adversarially Robust Policy&lt;/strong>: Learn a secure policy from jailbreaking.&lt;/li>
&lt;/ul>
&lt;p>What&amp;rsquo;s your own project idea?&lt;/p>
&lt;h3 id="keywords">Keywords&lt;/h3>
&lt;ul>
&lt;li>Vision-Language-Action Models (VLAs)&lt;/li>
&lt;li>Reinforcement Learning (RL)&lt;/li>
&lt;li>Selective Prediction&lt;/li>
&lt;li>Conformal Prediction&lt;/li>
&lt;li>Uncertainty Quantification&lt;/li>
&lt;/ul>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.usenix.org/conference/usenixsecurity23/presentation/park" target="_blank" rel="noopener">Security'23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2506.14067" target="_blank" rel="noopener">arXiv'25&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Trustworthy Code Generation</title><link>http://localhost:1313/research/coegen/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/research/coegen/</guid><description>&lt;p>Large code models (LCMs) are popularized as large language models (LLMs) are performant. LCMs may be treated as the same way as the LLMs, but we claim that LCMs have its unique &lt;strong>executable property&lt;/strong>! Would this unique property helps to build LCMs more functional or vulnerability-free? More precisely,&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>How to learn large code models (LCMs) that are &lt;mark>functional&lt;/mark> (i.e., satisfy user specification) and &lt;mark>free-from vulnerability&lt;/mark> by leveraging the executable property of code?&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>We leverage traditional static or dynamic program analysis methods, including fuzzing, to specify functional specification (e.g., &lt;a href="https://arxiv.org/abs/2505.13553" target="_blank" rel="noopener">arXiv'25&lt;/a>). Could we make this better?&lt;/p>
&lt;h3 id="on-goingpotential-projects">On-going/Potential Projects&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Code Generator Learning&lt;/strong>: Align LCMs via RL with unit tests.&lt;/li>
&lt;li>&lt;strong>Patch Generation&lt;/strong>: Learn agentic AI with tool calling to fix github issues.&lt;/li>
&lt;li>&lt;strong>Security Patch Generation&lt;/strong>: Learn agentic AI with tool calling to fix security vulnerabilities.&lt;/li>
&lt;/ul>
&lt;p>Do you have creative ideas in making LCMs better or using LCMs for solving other tasks?&lt;/p>
&lt;h3 id="keywords">Keywords&lt;/h3>
&lt;ul>
&lt;li>Code Generator&lt;/li>
&lt;li>Selective Prediction&lt;/li>
&lt;li>Conformal Prediction&lt;/li>
&lt;/ul>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://arxiv.org/abs/2505.13553" target="_blank" rel="noopener">arXiv'25&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.darpa.mil/news/2025/aixcc-results" target="_blank" rel="noopener">AIxCC-Winner&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Trustworthy LLMs</title><link>http://localhost:1313/research/trustllm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/research/trustllm/</guid><description>&lt;p>Large language models are amazingly performant and we use them in our daily life. Simultaneously, they raise many practical issues, including hallucination and harmful responses. This raises the following question.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>How to mitigate the &lt;mark>hallucination&lt;/mark>, &lt;mark>safety&lt;/mark>, &lt;mark>security&lt;/mark>, and &lt;mark>bias&lt;/mark> problems of large language models (LLMs) or large reasoning models (LRMs)?&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>LLMs confidently generate wrong, biased, and harmful information, which undermines the trust of LLMs as a knowledge base.
How to mitigate this?
One way could be leveraging conformal prediction and selective prediction to measure uncertainty as a basis for trust (e.g., &lt;a href="https://arxiv.org/abs/2307.09254" target="_blank" rel="noopener">NeurIPS'24&lt;/a>).
What other possibilities?&lt;/p>
&lt;h3 id="on-goingpotential-projects">On-going/Potential Projects&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Hallucination mitigation of LLM/LRMs&lt;/strong>: Mitigate the hallucination of LRMs.&lt;/li>
&lt;li>&lt;strong>Natural hamfulness mitigation of LLM/LRMs&lt;/strong>: Mitigate the safety of LRMs from natrual jailbreaking.&lt;/li>
&lt;li>&lt;strong>Adversarial hamfulness mitigation of LLM/LRMs&lt;/strong>: Mitigate the security of LRMs from adversarial jailbreaking.&lt;/li>
&lt;li>&lt;strong>Uncertainty quantification of LLM/LRMs&lt;/strong>: Quantify the correctness of the answers of LLMs or LRMs.&lt;/li>
&lt;li>&lt;strong>RL-based Alignment&lt;/strong>: Leverage bandits and Reinforcement Learning (RL) for building trustworthy LLMs.&lt;/li>
&lt;li>&lt;strong>Trustworthiness Control&lt;/strong>: Leverage theoretical results of bandits or RL to guarantee the trustworthiness.&lt;/li>
&lt;/ul>
&lt;p>What&amp;rsquo;s your own project idea?&lt;/p>
&lt;h3 id="keywords">Keywords&lt;/h3>
&lt;ul>
&lt;li>LLMs&lt;/li>
&lt;li>LRMs&lt;/li>
&lt;li>Selective Prediction&lt;/li>
&lt;li>Conformal Prediction&lt;/li>
&lt;li>Uncertainty Quantification&lt;/li>
&lt;/ul>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://openreview.net/forum?id=BJxVI04YvB" target="_blank" rel="noopener">ICLR'20&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://proceedings.mlr.press/v108/park20b/park20b.pdf" target="_blank" rel="noopener">AISTATS'20&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openreview.net/forum?id=Qk-Wq5AIjpq" target="_blank" rel="noopener">ICLR'21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openreview.net/pdf?id=DhP9L8vIyLc" target="_blank" rel="noopener">ICLR'22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2204.07482" target="_blank" rel="noopener">arXiv'22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openreview.net/forum?id=s6ygs1UCOw1" target="_blank" rel="noopener">NeurIPS'22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.usenix.org/conference/usenixsecurity23/presentation/park" target="_blank" rel="noopener">Security'23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2307.09254" target="_blank" rel="noopener">NeurIPS'24&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2506.14067" target="_blank" rel="noopener">arXiv'25&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2505.13553" target="_blank" rel="noopener">arXiv'25&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>üßë‚Äç‚öñÔ∏è Area Chair</title><link>http://localhost:1313/news/20251213-icml-ac/</link><pubDate>Sat, 13 Dec 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20251213-icml-ac/</guid><description/></item><item><title>Ensuring Functional Correctness of Large Code Models with Selective Generation</title><link>http://localhost:1313/publication/2025-12-01-ensuring-functional-correctness-of-large-code-models-with-selecti/</link><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-12-01-ensuring-functional-correctness-of-large-code-models-with-selecti/</guid><description/></item><item><title>Ensuring Functional Correctness of Large Code Models with Selective Generation</title><link>http://localhost:1313/publication_old/jeong-2025-ensuringfunctionalcorrectnesslarge/</link><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication_old/jeong-2025-ensuringfunctionalcorrectnesslarge/</guid><description>&lt;p>Add the &lt;strong>full text&lt;/strong> or &lt;strong>supplementary notes&lt;/strong> for the publication here using Markdown formatting.&lt;/p></description></item><item><title>ChronoBias: A Benchmark for Evaluating Time-conditional Group Bias in the Time-sensitive Knowledge of Large Language Models</title><link>http://localhost:1313/publication/2025-11-01-chronobias-a-benchmark-for-evaluating-time-conditional-group-bias/</link><pubDate>Sat, 01 Nov 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-11-01-chronobias-a-benchmark-for-evaluating-time-conditional-group-bias/</guid><description/></item><item><title>Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning</title><link>http://localhost:1313/publication/2025-10-01-holistic-unlearning-benchmark-a-multi-faceted-evaluation-for-text/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-10-01-holistic-unlearning-benchmark-a-multi-faceted-evaluation-for-text/</guid><description/></item><item><title>Retrieval-Augmented Generation with Estimation of Source Reliability</title><link>http://localhost:1313/publication/2025-10-01-retrieval-augmented-generation-with-estimation-of-source-reliabil/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-10-01-retrieval-augmented-generation-with-estimation-of-source-reliabil/</guid><description/></item><item><title>AI Kill Switch for Malicious Web-based LLM Agent</title><link>http://localhost:1313/publication/2025-09-26-ai-kill-switch-for-malicious-web-based-llm-agent/</link><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-09-26-ai-kill-switch-for-malicious-web-based-llm-agent/</guid><description/></item><item><title>üéâ One NeurIPS'25 Workshop Paper</title><link>http://localhost:1313/news/20250922-neurips-paper/</link><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250922-neurips-paper/</guid><description>&lt;p>Congrat to Jaewoo!&lt;/p>
&lt;p>Check out our paper on &lt;a href="https://arxiv.org/abs/2505.13553" target="_blank" rel="noopener">Ensuring Functional Correctness of Large Code Models with Selective Generation&lt;/a>.&lt;/p></description></item><item><title>ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System</title><link>http://localhost:1313/publication/2025-09-18-atlantis-ai-driven-threat-localization-analysis-and-triage-intell/</link><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-09-18-atlantis-ai-driven-threat-localization-analysis-and-triage-intell/</guid><description/></item><item><title>üßë‚Äç‚öñÔ∏è Area Chair</title><link>http://localhost:1313/news/20250905-iclr-ac/</link><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250905-iclr-ac/</guid><description/></item><item><title>üéâ Two EMNLP Papers</title><link>http://localhost:1313/news/20250821-emnlp/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250821-emnlp/</guid><description>&lt;p>Congrat to Kyungmin and Jeongyeon!&lt;/p></description></item><item><title>üèÜ DARPA AIxCC Winner!</title><link>http://localhost:1313/news/20250808-2-aixcc-final/</link><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250808-2-aixcc-final/</guid><description>&lt;p>Our &lt;a href="https://team-atlanta.github.io/" target="_blank" rel="noopener">Team Atlanta&lt;/a> won &lt;a href="https://aicyberchallenge.com/" target="_blank" rel="noopener">DARPA AIxCC&lt;/a> with a $4M award &amp;ndash; &lt;a href="https://www.darpa.mil/news/2025/aixcc-results" target="_blank" rel="noopener">official announcement&lt;/a>. This team consists of Samsung, GaTech, KAIST, and POSTECH (us!), led by Taesoo Kim (Samsung and GaTech). I led the POSTECH team, which includes my students, Minjae Gwon (POSTECH CSE) and Minjae Lee (POSTECH AI). My team focuses on patch generation mainly leveraging custom models. We will continue our journey to build &lt;strong>trustworthy&lt;/strong> patch/code generation toward practical applications.&lt;/p>
&lt;p>I&amp;rsquo;m fortunate to be in this amazing team and grateful to Prof. Taesoo Kim who built and led this amazing team!&lt;/p>
&lt;p>POSTECH GSAI supports GPUs for our team.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="POSTECH" srcset="
/news/20250808-2-aixcc-final/AIxCC-POSTECH_hu_6d1233b4ff884e57.webp 400w,
/news/20250808-2-aixcc-final/AIxCC-POSTECH_hu_c43db5e52334886f.webp 760w,
/news/20250808-2-aixcc-final/AIxCC-POSTECH_hu_70fc609b076fa86c.webp 1200w"
src="http://localhost:1313/news/20250808-2-aixcc-final/AIxCC-POSTECH_hu_6d1233b4ff884e57.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://img.global.news.samsung.com/global/wp-content/uploads/2025/08/Samsung-Corporate-Technology-AIxCC-First-Place-in-AI-Cyber-Challenge_main1.jpg" alt="Samsung" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>üèÜ Best Paper Finalist</title><link>http://localhost:1313/news/20250807-1-ckaia-award/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250807-1-ckaia-award/</guid><description>&lt;p>Congrat to Minjae and Yoonjae!
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="awardfig1" srcset="
/news/20250807-1-ckaia-award/fig1_hu_a5cd4b48f44c9eac.webp 400w,
/news/20250807-1-ckaia-award/fig1_hu_e4c51860f3d11472.webp 760w,
/news/20250807-1-ckaia-award/fig1_hu_8f93eb3df1b80220.webp 1200w"
src="http://localhost:1313/news/20250807-1-ckaia-award/fig1_hu_a5cd4b48f44c9eac.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="awardfig2" srcset="
/news/20250807-1-ckaia-award/fig2_hu_8f14930ff0982b60.webp 400w,
/news/20250807-1-ckaia-award/fig2_hu_2f6ecf7d5ee6034.webp 760w,
/news/20250807-1-ckaia-award/fig2_hu_b9f3bd83486ec5b1.webp 1200w"
src="http://localhost:1313/news/20250807-1-ckaia-award/fig2_hu_8f14930ff0982b60.webp"
width="570"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>Online Selective Generation with Adversarial Bandit Feedback</title><link>http://localhost:1313/publication/2025-07-01-online-selective-generation-with-adversarial-bandit-feedback/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-07-01-online-selective-generation-with-adversarial-bandit-feedback/</guid><description/></item><item><title>üéâ One ICCV'25 Paper</title><link>http://localhost:1313/news/20250626-iccv/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250626-iccv/</guid><description>&lt;p>Congrat to Saemi and Minjong!&lt;/p></description></item><item><title>SOUNDBOOST: Effective RCA and Attack Detection for UAV via Acoustic Side-Channel</title><link>http://localhost:1313/publication/2025-06-01-soundboost-effective-rca-and-attack-detection-for-uav-via-acousti/</link><pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2025-06-01-soundboost-effective-rca-and-attack-detection-for-uav-via-acousti/</guid><description/></item><item><title>üéâ One DSN'25 Paper</title><link>http://localhost:1313/news/20250520-dsn/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250520-dsn/</guid><description/></item><item><title>üèÜ Graduate Fellowship</title><link>http://localhost:1313/news/20250416-fellowship/</link><pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250416-fellowship/</guid><description>&lt;p>Congrat to Jaewoo!&lt;/p>
&lt;p>&lt;a href="">award&lt;/a>&lt;/p></description></item><item><title>üìö Trustworthy ML (2025 Spring)</title><link>http://localhost:1313/teaching/2025-tml-spring/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/teaching/2025-tml-spring/</guid><description>&lt;h1 id="trustworthy-ml-aigs703l--csed703l">Trustworthy ML (AIGS703L / CSED703L)&lt;/h1>
&lt;p>As machine learning and deep learning models are impacting on real world environments (e.g., ChatGPT), concerns on the trustworthiness of machine-learned models are rising. In this course, we explore whether popular machine-learned models are trustworthy and then study various learning methods to enchant the models to be trustworthy. To this end, we will learn basic knowledge on machine learning theory, uncertainty learning via conformal prediction, adversarial examples/learning, machine unlearning, differentially private learning, fairness in learning, and miscellaneous topics on trustworthy generative AI.&lt;/p>
&lt;h3 id="location">Location&lt;/h3>
&lt;p>B2 Room 107&lt;/p>
&lt;h3 id="instructor">Instructor&lt;/h3>
&lt;p>Sangdon Park&lt;/p>
&lt;h3 id="teaching-assistant">Teaching Assistant&lt;/h3>
&lt;p>Byeonggyu Kim (&lt;a href="mailto:qudrb6989@postech.ac.kr">qudrb6989@postech.ac.kr&lt;/a>)&lt;/p>
&lt;h3 id="schedule">Schedule&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;div style="width:100px">Date&lt;/div>&lt;/th>
&lt;th>Topic&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>[W1] 2/18&lt;/td>
&lt;td>&lt;a href="./notes/0-intro.pdf">Trustworthy ML Introduction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W1] 2/20&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-measure.pdf">Measure Theory: Introduction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W2] 2/25&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-pac.pdf">Learning Theory: PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W2] 2/27&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-pac.pdf">Learning Theory: PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W3] 3/4&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-uc.pdf">Learning Theory: Beyond PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W3] 3/6&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-uc.pdf">Learning Theory: Beyond PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W4] 3/11&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-online.pdf">Learning Theory: Online learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W4] 3/13&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-online.pdf">Learning Theory: Online learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W5] 3/18&lt;/td>
&lt;td>&lt;a href="./notes/2-cp.pdf">Controllable Uncertainty Learning: Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W5] 3/20&lt;/td>
&lt;td>&lt;a href="./notes/2-cp.pdf">Controllable Uncertainty Learning: Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W6] 3/25&lt;/td>
&lt;td>&lt;a href="./notes/2-paccp.pdf">Controllable Uncertainty Learning: PAC Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W6] 3/27&lt;/td>
&lt;td>&lt;a href="./notes/2-acp.pdf">Controllable Uncertainty Learning: Adaptive Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W7] 4/1&lt;/td>
&lt;td>&lt;a href="./notes/2-sp.pdf">Controllable Uncertainty Learning: Selective Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W7] 4/3&lt;/td>
&lt;td>&lt;a href="./notes/3-ae.pdf">Adversarial Learning: Adversarial Examples and Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W8] 4/8&lt;/td>
&lt;td>&lt;a href="./notes/3-cert.pdf">Adversarial Learning: Certified Adversarial Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W8] 4/10&lt;/td>
&lt;td>&lt;a href="./notes/3-cert.pdf">Adversarial Learning: Certified Adversarial Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W9] 4/15&lt;/td>
&lt;td>&lt;a href="./notes/5-dp1.pdf">Differential Privacy: Basics&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W9] 4/17&lt;/td>
&lt;td>&lt;a href="./notes/5-dp1.pdf">Differential Privacy: Basics&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W10] 4/22&lt;/td>
&lt;td>&lt;a href="./notes/5-dp2.pdf">Differential Privacy: Practice&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W10] 4/24&lt;/td>
&lt;td>&lt;a href="./notes/4-cul.pdf">Machine Unlearning: Linear Models&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W11] 4/29&lt;/td>
&lt;td>&lt;a href="./notes/4-cul.pdf">Machine Unlearning: Linear Models&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W11] 5/1&lt;/td>
&lt;td>&lt;a href="./notes/4-cul2.pdf">Machine Unlearning: Deep Models&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W12] 5/6&lt;/td>
&lt;td>Children&amp;rsquo;s Day (Extended)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W12] 5/8&lt;/td>
&lt;td>&lt;a href="./notes/6-fair1.pdf">Fairness in Learning: Foundation&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W13] 5/13&lt;/td>
&lt;td>&lt;a href="">Miscellaneous Topics in TML&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W13] 5/15&lt;/td>
&lt;td>Student Presentation 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W14] 5/20&lt;/td>
&lt;td>Student Presentation 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W14] 5/22&lt;/td>
&lt;td>Student Presentation 3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W15] 5/27&lt;/td>
&lt;td>Student Presentation 4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W15] 5/29&lt;/td>
&lt;td>Student Presentation 5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W16] 6/3&lt;/td>
&lt;td>Student Presentation 6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[W16] 6/5&lt;/td>
&lt;td>Final Exam&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>üßë‚Äç‚öñÔ∏è Area Chair</title><link>http://localhost:1313/news/20240327-neurips-ac/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240327-neurips-ac/</guid><description/></item><item><title>üèÜ Best Paper Award</title><link>http://localhost:1313/news/20250106-award/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250106-award/</guid><description>&lt;p>Congrat to Kyungmin and Minjae!&lt;/p></description></item><item><title>üéâ 2024 Summary</title><link>http://localhost:1313/news/20241231-summary/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20241231-summary/</guid><description>&lt;p>I appreciate all of my collaborators!&lt;/p></description></item><item><title>Selective Generation for Controllable Language Models</title><link>http://localhost:1313/publication/2024-12-01-selective-generation-for-controllable-language-models/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2024-12-01-selective-generation-for-controllable-language-models/</guid><description/></item><item><title>üßë‚Äç‚öñÔ∏è Area Chair</title><link>http://localhost:1313/news/20241122-icml-ac/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20241122-icml-ac/</guid><description/></item><item><title>üéâ One NeurIPS'24 Paper (Spotlight)</title><link>http://localhost:1313/news/20240926-neurips-paper/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240926-neurips-paper/</guid><description>&lt;p>Congrat to Minjae and Kyungmin!&lt;/p></description></item><item><title>üé§ Invite Talk</title><link>http://localhost:1313/news/20240910-invited-talk/</link><pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240910-invited-talk/</guid><description/></item><item><title>üé§ Breakout Session</title><link>http://localhost:1313/news/20240909-reaim/</link><pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240909-reaim/</guid><description>&lt;p>Check out &lt;a href="https://reaim2024.kr/" target="_blank" rel="noopener">Responsible AI in the Military Domain (REAIM)&lt;/a> Summit.&lt;/p></description></item><item><title>üìö Trustworthy ML (2024 Fall)</title><link>http://localhost:1313/teaching/2024-tml-fall/</link><pubDate>Tue, 03 Sep 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/teaching/2024-tml-fall/</guid><description>&lt;h1 id="trustworthy-ml-aigs703l--csed703l">Trustworthy ML (AIGS703L / CSED703L)&lt;/h1>
&lt;p>As machine learning and deep learning models are impacting on real world environments (e.g., ChatGPT), concerns on the trustworthiness of machine-learned models are rising. In this course, we explore whether popular machine-learned models are trustworthy and then study various learning methods to enchant the models to be trustworthy. To this end, we will learn basic knowledge on machine learning theory, uncertainty learning via conformal prediction, adversarial examples/learning, machine unlearning, differentially private learning, fairness in learning, and miscellaneous topics on trustworthy generative AI.&lt;/p>
&lt;h3 id="location">Location&lt;/h3>
&lt;p>TJ Park Lib. Room 501&lt;/p>
&lt;h3 id="instructor">Instructor&lt;/h3>
&lt;p>Sangdon Park&lt;/p>
&lt;h3 id="teaching-assistant">Teaching Assistant&lt;/h3>
&lt;p>Kyungmin Kim (&lt;a href="mailto:kkm959595@postech.ac.kr">kkm959595@postech.ac.kr&lt;/a>)&lt;/p>
&lt;h3 id="schedule">Schedule&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;div style="width:150px">Date&lt;/div>&lt;/th>
&lt;th>Topic&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>[Week 1] 9/3&lt;/td>
&lt;td>&lt;a href="./notes/0-intro.pdf">Trustworthy ML Introduction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 1] 9/5&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-measure.pdf">Measure Theory: Introduction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 2] 9/10&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-pac.pdf">Learning Theory: PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 2] 9/12&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-uc.pdf">Learning Theory: Beyond PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 3] 9/17&lt;/td>
&lt;td>Korean Thanksgiving holiday&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 3] 9/19&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-uc.pdf">Learning Theory: Beyond PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 4] 9/24&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-online.pdf">Learning Theory: Online learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 4] 9/26&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-online.pdf">Learning Theory: Online learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 5] 10/1&lt;/td>
&lt;td>Armed Forces Day&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 5] 10/3&lt;/td>
&lt;td>National Foundation Day&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 6] 10/8&lt;/td>
&lt;td>&lt;a href="./notes/2-cp.pdf">Controllable Uncertainty Learning: Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 6] 10/10&lt;/td>
&lt;td>&lt;a href="./notes/2-paccp.pdf">Controllable Uncertainty Learning: PAC Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 7] 10/15&lt;/td>
&lt;td>&lt;a href="./notes/2-acp.pdf">Controllable Uncertainty Learning: Adaptive Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 7] 10/17&lt;/td>
&lt;td>&lt;a href="./notes/2-sp.pdf">Controllable Uncertainty Learning: Selective Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 8] 10/22&lt;/td>
&lt;td>&lt;a href="./notes/3-ae.pdf">Adversarial Learning: Adversarial Examples and Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 8] 10/24&lt;/td>
&lt;td>&lt;a href="./notes/3-cert.pdf">Adversarial Learning: Certified Adversarial Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 9] 10/29&lt;/td>
&lt;td>&lt;a href="./notes/5-dp1.pdf">Differential Privacy: Basics&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 9] 10/31&lt;/td>
&lt;td>&lt;a href="./notes/5-dp2.pdf">Differential Privacy: Practice&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 10] 11/5&lt;/td>
&lt;td>&lt;a href="./notes/4-cul.pdf">Machine Unlearning: Linear Models&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 10] 11/7&lt;/td>
&lt;td>&lt;a href="./notes/4-cul2.pdf">Machine Unlearning: Deep Models&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 11] 11/12&lt;/td>
&lt;td>&lt;a href="./notes/6-fair1.pdf">Fairness in Learning: Foundation&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 11] 11/14&lt;/td>
&lt;td>[Fairness in Learning: NLP Application]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 12] 11/19&lt;/td>
&lt;td>Miscellaneous Topics in TML&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 12] 11/21&lt;/td>
&lt;td>Final Exam&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 13] 11/26&lt;/td>
&lt;td>Student Discussion 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 13] 11/28&lt;/td>
&lt;td>Student Discussion 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 14] 12/3&lt;/td>
&lt;td>Student Discussion 3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 14] 12/5&lt;/td>
&lt;td>Student Discussion 4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 15] 12/10&lt;/td>
&lt;td>Student Discussion 5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 15] 12/12&lt;/td>
&lt;td>Student Discussion 6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 16] 12/17&lt;/td>
&lt;td>Student Discussion 7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 16] 12/19&lt;/td>
&lt;td>Student Discussion 8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>üèÜ DARPA AIxCC Semifinals</title><link>http://localhost:1313/news/20240812-aixcc-semifinals/</link><pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240812-aixcc-semifinals/</guid><description>&lt;p>&lt;a href="https://team-atlanta.github.io/" target="_blank" rel="noopener">Team Atlanta&lt;/a> advances to the &lt;a href="https://aicyberchallenge.com/" target="_blank" rel="noopener">DARPA AIxCC&lt;/a> finals with a $2M team award.&lt;/p></description></item><item><title>MedBN: Robust Test Time Adaptation against Malicious Test Samples</title><link>http://localhost:1313/publication/2024-06-01-medbn-robust-test-time-adaptation-against-malicious-test-samples/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2024-06-01-medbn-robust-test-time-adaptation-against-malicious-test-samples/</guid><description/></item><item><title>TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction</title><link>http://localhost:1313/publication/2024-06-01-traq-trustworthy-retrieval-augmented-question-answering-via-confo/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2024-06-01-traq-trustworthy-retrieval-augmented-question-answering-via-confo/</guid><description/></item><item><title>PAC Prediction Sets Under Label Shift</title><link>http://localhost:1313/publication/2024-05-01-pac-prediction-sets-under-label-shift/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2024-05-01-pac-prediction-sets-under-label-shift/</guid><description/></item><item><title>üßë‚Äç‚öñÔ∏è Area Chair</title><link>http://localhost:1313/news/20250217-neurips-ac/</link><pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20250217-neurips-ac/</guid><description/></item><item><title>üéâ One NAACL'24 Paper</title><link>http://localhost:1313/news/20240313-naacl-paper/</link><pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240313-naacl-paper/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2307.04642" target="_blank" rel="noopener">TRAQ&lt;/a> is accepted to NAACL‚Äô24.&lt;/p></description></item><item><title>üéâ One CVPR'24 Paper</title><link>http://localhost:1313/news/20240227-cvpr-paper/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240227-cvpr-paper/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.19326" target="_blank" rel="noopener">MedBN&lt;/a> is accepted to CVPR'24.&lt;/p></description></item><item><title>üìö Discrete Mathematics (2024 Sring)</title><link>http://localhost:1313/teaching/2024-discrete-math-spring/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/teaching/2024-discrete-math-spring/</guid><description>&lt;h1 id="discrete-mathematics-csed261">Discrete Mathematics (CSED261)&lt;/h1>
&lt;p>&lt;a href="https://plms.postech.ac.kr/course/view.php?id=9586" target="_blank" rel="noopener">Class Page&lt;/a>&lt;/p></description></item><item><title>üéâ One ICLR'24 Paper</title><link>http://localhost:1313/news/20240116-iclr-paper/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20240116-iclr-paper/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.12964" target="_blank" rel="noopener">Label Shift&lt;/a> is accepted to ICLR'24.&lt;/p></description></item><item><title>Angelic Patches for Improving Third-Party Object Detector Performance</title><link>http://localhost:1313/publication/2023-12-01-angelic-patches-for-improving-third-party-object-detector-perform/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2023-12-01-angelic-patches-for-improving-third-party-object-detector-perform/</guid><description/></item><item><title>üìö Trustworthy ML (2023 Fall)</title><link>http://localhost:1313/teaching/2023-tml-fall/</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/teaching/2023-tml-fall/</guid><description>&lt;h1 id="trustworthy-ml-aigs703l--csed703l">Trustworthy ML (AIGS703L / CSED703L)&lt;/h1>
&lt;p>As machine learning and deep learning models are impacting on real world environments (e.g., ChatGPT), concerns on the trustworthiness of machine-learned models are rising. In this course, we explore whether popular machine-learned models are trustworthy and then study various learning methods to enchant the models to be trustworthy. To this end, we will learn basic knowledge on machine learning theory, uncertainty learning via conformal prediction, adversarial examples/learning, machine unlearning, differentially private learning, fairness in learning, and miscellaneous topics on trustworthy generative AI.&lt;/p>
&lt;h3 id="location">Location&lt;/h3>
&lt;p>B2 Room 107&lt;/p>
&lt;h3 id="instructor">Instructor&lt;/h3>
&lt;p>Sangdon Park&lt;/p>
&lt;h3 id="schedule-tentative">Schedule (tentative)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;div style="width:150px">Date&lt;/div>&lt;/th>
&lt;th>Topic&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>[Week 1] 9/5&lt;/td>
&lt;td>&lt;a href="./notes/0-intro.pdf">Trustworthy ML Introduction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 1] 9/7&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-pac.pdf">Learning Theory: PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 2] 9/12&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-uc.pdf">Learning Theory: Beyond PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 2] 9/14&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-uc.pdf">Learning Theory: Beyond PAC learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 3] 9/19&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-online.pdf">Learning Theory: Online learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 3] 9/21&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-online.pdf">Learning Theory: Online learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 4] 9/26&lt;/td>
&lt;td>&lt;a href="./notes/1-theory-sq.pdf">Learning Theory: Statistical Query&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 4] 9/28&lt;/td>
&lt;td>Korean Thanksgiving holiday&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 5] 10/3&lt;/td>
&lt;td>National Foundation Day&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 5] 10/5&lt;/td>
&lt;td>&lt;a href="./notes/2-cp.pdf">Uncertainty Learning: Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 6] 10/10&lt;/td>
&lt;td>&lt;a href="./notes/2-paccp.pdf">Uncertainty Learning: PAC Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 6] 10/12&lt;/td>
&lt;td>&lt;a href="./notes/2-acp.pdf">Uncertainty Learning: Adaptive Conformal Prediction&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 7] 10/17&lt;/td>
&lt;td>Student Presentation 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 7] 10/19&lt;/td>
&lt;td>&lt;a href="./notes/3-ae.pdf">Adversarial Learning: Adversarial Example / Heuristic Adversarial Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 8] 10/24&lt;/td>
&lt;td>&lt;a href="./notes/3-cert.pdf">Adversarial Learning: Certified Adversarial Learning&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 8] 10/26&lt;/td>
&lt;td>&lt;a href="./notes/4-cul.pdf">Machine Unlearning 1&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 9] 10/31&lt;/td>
&lt;td>Student Presentation 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 9] 11/2&lt;/td>
&lt;td>&lt;a href="./notes/4-cul2.pdf">Machine Unlearning 2&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 10] 11/7&lt;/td>
&lt;td>&lt;a href="./notes/5-dp1.pdf">Differential Privacy 1&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 10] 11/9&lt;/td>
&lt;td>Student Presentation 3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 11] 11/14&lt;/td>
&lt;td>Student Presentation 3.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 11] 11/16&lt;/td>
&lt;td>&lt;a href="./notes/5-dp2.pdf">Differential Privacy 2&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 12] 11/21&lt;/td>
&lt;td>&lt;a href="./notes/6-fair1.pdf">Fairness in Learning 1&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 12] 11/23&lt;/td>
&lt;td>&lt;a href="./notes/6-fair2.pdf">Fairness in Learning 2&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 13] 11/28&lt;/td>
&lt;td>Student Presentation 4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 13] 11/30&lt;/td>
&lt;td>Student Presentation 5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 14] 12/5&lt;/td>
&lt;td>&lt;a href="./notes/7-codegen.pdf">Code Generation&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 14] 12/7&lt;/td>
&lt;td>&lt;a href="./notes/7-copyright.pdf">Copyright&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 15] 12/12&lt;/td>
&lt;td>Student Presentation 6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 15] 12/14&lt;/td>
&lt;td>Student Presentation 7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 16] 12/19&lt;/td>
&lt;td>Student Presentation 8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>[Week 16] 12/21&lt;/td>
&lt;td>Student Presentation 9&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>üéâ Assistant Professor</title><link>http://localhost:1313/news/20230801-start/</link><pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20230801-start/</guid><description/></item><item><title>üèÜ Best Paper Award</title><link>http://localhost:1313/news/20230801-award/</link><pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/news/20230801-award/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2307.04642" target="_blank" rel="noopener">TRAC&lt;/a> got a Best Paper award at ICML23 TEACH Workshop.&lt;/p></description></item><item><title>ACon$^2$: Adaptive Conformal Consensus for Provable Blockchain Oracles</title><link>http://localhost:1313/publication/2023-08-01-acon-2-adaptive-conformal-consensus-for-provable-blockchain-oracl/</link><pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2023-08-01-acon-2-adaptive-conformal-consensus-for-provable-blockchain-oracl/</guid><description/></item><item><title>CODiT: Conformal Out-of-Distribution Detection in Time-Series Data for Cyber-Physical Systems</title><link>http://localhost:1313/publication/2023-05-01-codit-conformal-out-of-distribution-detection-in-time-series-data/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2023-05-01-codit-conformal-out-of-distribution-detection-in-time-series-data/</guid><description/></item><item><title>PAC Prediction Sets for Meta-Learning</title><link>http://localhost:1313/publication/2022-12-01-pac-prediction-sets-for-meta-learning/</link><pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2022-12-01-pac-prediction-sets-for-meta-learning/</guid><description/></item><item><title>Unsafe's Betrayal: Abusing Unsafe Rust in Binary Reverse Engineering via Machine Learning</title><link>http://localhost:1313/publication/2022-10-31-unsafe-s-betrayal-abusing-unsafe-rust-in-binary-reverse-engineeri/</link><pubDate>Mon, 31 Oct 2022 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2022-10-31-unsafe-s-betrayal-abusing-unsafe-rust-in-binary-reverse-engineeri/</guid><description/></item><item><title>Sequential Covariate Shift Detection Using Classifier Two-Sample Tests</title><link>http://localhost:1313/publication/2022-07-01-sequential-covariate-shift-detection-using-classifier-two-sample-/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2022-07-01-sequential-covariate-shift-detection-using-classifier-two-sample-/</guid><description/></item><item><title>Towards PAC Multi-Object Detection and Tracking</title><link>http://localhost:1313/publication/2022-04-15-towards-pac-multi-object-detection-and-tracking/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2022-04-15-towards-pac-multi-object-detection-and-tracking/</guid><description/></item><item><title>PAC Prediction Sets Under Covariate Shift</title><link>http://localhost:1313/publication/2022-04-01-pac-prediction-sets-under-covariate-shift/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2022-04-01-pac-prediction-sets-under-covariate-shift/</guid><description/></item><item><title>PAC Confidence Predictions for Deep Neural Network Classifiers</title><link>http://localhost:1313/publication/2021-05-01-pac-confidence-predictions-for-deep-neural-network-classifiers/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2021-05-01-pac-confidence-predictions-for-deep-neural-network-classifiers/</guid><description/></item><item><title>iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection</title><link>http://localhost:1313/publication/2021-02-01-idecode-in-distribution-equivariance-for-conformal-out-of-distrib/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2021-02-01-idecode-in-distribution-equivariance-for-conformal-out-of-distrib/</guid><description/></item><item><title>Calibrated prediction with covariate shift via unsupervised domain adaptation</title><link>http://localhost:1313/publication/2020-08-01-calibrated-prediction-with-covariate-shift-via-unsupervised-domai/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2020-08-01-calibrated-prediction-with-covariate-shift-via-unsupervised-domai/</guid><description/></item><item><title>Calibrated prediction with covariate shift via unsupervised domain adaptation</title><link>http://localhost:1313/publication_old/park-2020-calibrated/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication_old/park-2020-calibrated/</guid><description>&lt;p>Add the &lt;strong>full text&lt;/strong> or &lt;strong>supplementary notes&lt;/strong> for the publication here using Markdown formatting.&lt;/p></description></item><item><title>PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction</title><link>http://localhost:1313/publication/2020-04-01-pac-confidence-sets-for-deep-neural-networks-via-calibrated-predi/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2020-04-01-pac-confidence-sets-for-deep-neural-networks-via-calibrated-predi/</guid><description/></item><item><title>PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction</title><link>http://localhost:1313/publication_old/park-2020-pac/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication_old/park-2020-pac/</guid><description>&lt;p>Add the &lt;strong>full text&lt;/strong> or &lt;strong>supplementary notes&lt;/strong> for the publication here using Markdown formatting.&lt;/p></description></item><item><title>Resilient linear classification: an approach to deal with attacks on training data</title><link>http://localhost:1313/publication/2017-04-01-resilient-linear-classification-an-approach-to-deal-with-attacks-/</link><pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2017-04-01-resilient-linear-classification-an-approach-to-deal-with-attacks-/</guid><description/></item><item><title>Integrated intelligence for human-robot teams</title><link>http://localhost:1313/publication/2016-10-01-integrated-intelligence-for-human-robot-teams/</link><pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2016-10-01-integrated-intelligence-for-human-robot-teams/</guid><description/></item><item><title>Abnormal object detection by canonical scene-based contextual model</title><link>http://localhost:1313/publication/2012-10-01-abnormal-object-detection-by-canonical-scene-based-contextual-mod/</link><pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate><guid>http://localhost:1313/publication/2012-10-01-abnormal-object-detection-by-canonical-scene-based-contextual-mod/</guid><description/></item><item><title>Admin</title><link/><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid/><description/></item><item><title>Intern</title><link/><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid/><description/></item><item><title>Team</title><link>http://localhost:1313/team/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/team/</guid><description/></item></channel></rss>