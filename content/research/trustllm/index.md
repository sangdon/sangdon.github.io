---
title: Trustworthy LLMs 
summary: Make LLMs trustful, fair, safe, and secue.
#date: 2025-02-18
reading_time: false
weight: 9

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'ChatGPT 4o'

authors:
  - Sangdon Park

#tags:
#  - Academic
#  - Markdown
  
share: false
toc: false

---


> **How to mitigate the <mark>hallucination</mark>, <mark>safety</mark>, <mark>security</mark>, and <mark>bias</mark> problems of LLMs?**


LMMs confidently generate wrong information, which undermines the trust of LLMs as a knowledge base.
How to mitigate this?
One way could be leveraging conformal prediction and selective prediction to measure uncertainty as a basis for trust (e.g., [NeurIPS'24](https://arxiv.org/abs/2307.09254)).
What other possibilities?


**Keywords**: *uncertainty quantification*, *conformal prediction*, *selective prediction*, *LLMs*

**Related Work**:
[ICLR'20](https://openreview.net/forum?id=BJxVI04YvB),
[AISTATS'20](http://proceedings.mlr.press/v108/park20b/park20b.pdf),
[ICLR'21](https://openreview.net/forum?id=Qk-Wq5AIjpq),
[ICLR'22](https://openreview.net/pdf?id=DhP9L8vIyLc),
[arXiv'22](https://arxiv.org/abs/2204.07482),
[NeurIPS'22](https://openreview.net/forum?id=s6ygs1UCOw1),
[Security'23](https://www.usenix.org/conference/usenixsecurity23/presentation/park),
[NeurIPS'24](https://arxiv.org/abs/2307.09254)
